# base
name: wcsac

# for RL
discount_factor: 0.99
n_steps: 10
n_update_iters: 10
batch_size: 256
len_replay_buffer: 1000000
max_grad_norm: 1.0
soft_update_ratio: 0.995
actor_lr: 1e-3
critic_lr: 1e-3

# for entropy
entropy_threshold: -1.0
ent_alpha_lr: 1e-3

# for constraint
con_threshold: 0.025
con_alpha: 0.25
con_damp: 10.0
con_beta_lr: 5e-2

# for backup
backup_files: [
    algos/wcsac/agent.py,
    algos/wcsac/storage.py,
    algos/wcsac/safetygym.yaml,
]

# for logging
logging:
    metric: [fps, entropy, constraint, ent_alpha, con_beta]
    rollout: [reward_sum, cost_sum, eplen]
    train: [cost_critic_loss, cost_std_critic_loss, reward_critic_loss, policy_loss]

# for model
model:
    actor:
        mlp:
            shape: [512, 512]
            activation: ReLU
        log_std_init: 0.0
    reward_critic:
        mlp:
            shape: [512, 512]
            activation: ReLU
        clip_range: [-np.inf, np.inf]
    cost_critic:
        mlp:
            shape: [512, 512]
            activation: ReLU
        clip_range: [-np.inf, np.inf]
    cost_std_critic:
        mlp:
            shape: [512, 512]
            activation: ReLU
        out_activation: softplus
        clip_range: [0.0, np.inf]
