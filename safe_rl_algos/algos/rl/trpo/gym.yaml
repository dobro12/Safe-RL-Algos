# base
name: trpo

# for RL
norm_obs: true
norm_reward: false
discount_factor: 0.99
n_steps: 5000
critic_lr: 3e-4
n_critic_iters: 20
max_grad_norm: 1.0
gae_coeff: 0.97

# for trust region
max_kl: 0.01
damping_coeff: 0.01
num_conjugate: 10
line_decay: 0.8
kl_tolerance: 2.0

# for backup
backup_files: [
    safe_rl_algos/algos/rl/trpo/agent.py,
    safe_rl_algos/algos/rl/trpo/storage.py,
    safe_rl_algos/algos/rl/trpo/gym.yaml,
]

# for logging
logging: [fps, reward_sum, eplen, beta, kl, entropy, objective, reward_critic_loss]

# for model
model:
    actor:
        mlp:
            shape: [512, 512]
            activation: LeakyReLU
        last_activation: Tanh
        use_action_bound: true
        log_std_init: 0.0
        log_std_fix: false
    reward_critic:
        mlp:
            shape: [512, 512]
            activation: LeakyReLU
        clip_range: [-np.inf, np.inf]
